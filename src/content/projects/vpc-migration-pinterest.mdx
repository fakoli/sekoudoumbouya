---
title: '$10M VPC Architecture Transformation'
slug: vpc-migration-pinterest
publishDate: '2024-06-15'
description: 'Eliminated $10M+ in annual Transit Gateway costs by redesigning Pinterest multi-region network architecture to use VPC peering with zero production downtime.'
category: strategic
role: 'Tech Lead / DRI'
organization: Pinterest
impactSummary: 'Led the complete redesign of Pinterest multi-region VPC architecture, eliminating Transit Gateway bottlenecks and saving $10M+ annually while maintaining zero-downtime migration across 4 AWS regions.'
scale:
  - 4 AWS regions
  - 500+ VPCs migrated
  - Petabytes of cross-region traffic
  - Zero production downtime
primaryTech:
  - AWS VPC Peering
  - Transit Gateway (legacy)
  - Terraform
  - Python
  - IPAM
  - Route 53 Resolver
contributions:
  - Designed direct VPC peering architecture replacing Transit Gateway
  - Built migration tooling for zero-downtime cutover
  - Created IPAM strategy for non-overlapping CIDR blocks
  - Implemented phased rollout across 4 regions
  - Documented runbooks and rollback procedures
outcomes:
  - '$10M+ annual infrastructure savings'
  - 'Zero production downtime during migration'
  - '40% reduction in cross-region latency'
  - 'Eliminated single point of failure in network architecture'
tags:
  - aws
  - networking
  - cost-optimization
  - multi-region
  - infrastructure
duration: '2023-Q1 to 2023-Q4'
featured: true

# Decision Card - Structured thinking summary
decisionCard:
  problem: 'Pinterest Transit Gateway costs were scaling linearly with traffic growth, projected to exceed $15M annually by 2024. The centralized TGW architecture created a single point of failure and added 2-3ms latency to every cross-VPC request. We needed a solution that would reduce costs while improving reliability and performance.'
  constraints:
    - 'Zero production downtime tolerance — Pinterest serves 450M+ monthly active users'
    - 'Must maintain existing security boundaries and compliance requirements'
    - 'Migration must be reversible at any point (rollback capability required)'
    - 'Cannot require application-level changes — transparent to service teams'
    - 'Must complete within 9-month budget cycle to realize savings in FY2024'
  tradeoffs:
    - option: 'Direct VPC Peering'
      pros:
        - 'Eliminates per-GB Transit Gateway charges'
        - 'Lower latency (removes TGW hop)'
        - 'Simpler failure domain'
      cons:
        - 'More peering connections to manage (n×n vs n)'
        - 'Requires non-overlapping CIDRs'
        - 'Route table complexity increases'
      chosen: true
    - option: 'AWS PrivateLink'
      pros:
        - 'Service-centric connectivity model'
        - 'Built-in endpoint policies for security'
      cons:
        - 'Per-hour and per-GB charges could exceed TGW'
        - 'Requires service discovery changes'
        - 'Not suitable for all traffic patterns'
      chosen: false
    - option: 'Keep Transit Gateway, negotiate pricing'
      pros:
        - 'No migration risk'
        - 'Familiar operational model'
      cons:
        - 'AWS pricing is non-negotiable at our scale'
        - 'Doesnt address latency or SPOF issues'
        - 'Costs continue to scale with growth'
      chosen: false
  artifact:
    type: diagram
    title: 'VPC Peering Architecture Diagram'
    preview: '/artifacts/vpc-architecture-thumbnail.png'
---

## Context

When I joined the Cloud Infrastructure team at Pinterest in 2021, the multi-region network architecture relied heavily on AWS Transit Gateway as the central hub for all VPC-to-VPC communication. This pattern made sense when it was implemented in 2019 — Transit Gateway simplified routing and provided a single place to apply security policies.

By 2023, the math had changed dramatically. Pinterest's traffic growth meant we were routing petabytes of data through Transit Gateway monthly, and AWS charges per-GB for that privilege. Our monthly TGW bill had grown to nearly $1M, with projections showing it would hit $1.25M by end of year if traffic trends continued.

Beyond cost, the centralized architecture created operational risks. Transit Gateway became a single point of failure — any TGW issue affected all cross-VPC communication. We also measured an additional 2-3ms latency on every cross-region request, which compounds significantly for services making multiple internal API calls.

## The Decision

After evaluating multiple options (detailed in the Decision Card above), I proposed replacing Transit Gateway with direct VPC peering for the majority of our cross-VPC traffic. This would:

1. **Eliminate per-GB charges** — VPC peering has no data transfer charges within the same region
2. **Reduce latency** — Direct peering removes the Transit Gateway hop
3. **Improve fault isolation** — Each peering connection is independent

The tradeoff was complexity: instead of managing `n` connections to a central hub, we'd manage `n×(n-1)/2` peer connections. For our 500+ VPCs, this meant thousands of peering connections.

## Migration Strategy

The key insight that made this feasible was recognizing we didn't need to migrate everything at once. I designed a phased approach:

### Phase 1: IPAM Foundation (Month 1-2)

Before any network changes, we needed to ensure non-overlapping CIDR blocks across all VPCs. I implemented an IPAM (IP Address Management) strategy using AWS VPC IPAM combined with custom Terraform modules to:

- Audit existing CIDR allocations and identify conflicts
- Establish regional CIDR pools with enough headroom for growth
- Create automated CIDR allocation for new VPCs

### Phase 2: Parallel Path (Month 3-5)

We established VPC peering connections *alongside* existing Transit Gateway routes. Traffic continued flowing through TGW while we:

- Created peering connections between high-traffic VPC pairs first
- Validated routing and security groups worked correctly
- Built monitoring dashboards to compare latency and throughput

### Phase 3: Traffic Migration (Month 6-8)

Using Route 53 Resolver rules and route table priorities, we gradually shifted traffic from TGW to VPC peering:

- Started with 5% of traffic to validate
- Monitored error rates, latency percentiles, and application metrics
- Increased percentage in steps: 5% → 25% → 50% → 75% → 100%
- Each step included a 48-hour bake period

### Phase 4: Cleanup (Month 9)

Once all traffic was flowing through VPC peering, we:

- Removed Transit Gateway attachments
- Updated documentation and runbooks
- Decommissioned the Transit Gateway infrastructure

## Results

The migration completed in Q4 2023 with the following outcomes:

- **$10M+ annual savings** — Monthly infrastructure costs dropped by ~$850K
- **Zero production incidents** — Not a single customer-facing issue during migration
- **40% latency improvement** — P95 cross-VPC latency dropped from 5ms to 3ms
- **Improved reliability** — Eliminated Transit Gateway as a single point of failure

## Key Learnings

**Invest in reversibility.** The parallel path approach meant we could roll back instantly if issues emerged. This reduced stakeholder anxiety and let us move faster with confidence.

**Automate the boring parts.** The CIDR audit and peering connection creation would have taken months manually. Terraform automation compressed this to weeks.

**Measure before and after.** Having baseline latency and error rate metrics made it easy to validate each migration step and communicate progress to leadership.

## Key Contributions

- **Designed direct VPC peering architecture** replacing centralized Transit Gateway with distributed peer mesh
- **Built comprehensive migration tooling** in Python and Terraform for zero-downtime traffic cutover
- **Created IPAM strategy** ensuring non-overlapping CIDR blocks across 500+ VPCs
- **Implemented phased rollout** with automated traffic shifting and instant rollback capability
- **Documented operational runbooks** for the new architecture, enabling team self-service
